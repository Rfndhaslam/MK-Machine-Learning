{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-zkuNkWMQM0Z"
      },
      "outputs": [],
      "source": [
        "# Bagian 1: Setup dan Pengolahan Data\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"heart.csv\")\n",
        "\n",
        "# Split features dan target\n",
        "X = data.drop(columns=['target'])\n",
        "y = data['target']\n",
        "\n",
        "# Normalize features untuk performa MLP yang lebih baik\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Bagi data menjadi training dan testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Konversi data menjadi PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)  # Target untuk klasifikasi\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagian 2: Definisi Model MLP\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Definisikan fungsi untuk membuat DataLoader\n",
        "def create_data_loader(X, y, batch_size):\n",
        "    dataset = TensorDataset(X, y)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Definisikan model MLP\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, activation_fn):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        current_size = input_size\n",
        "        for layer_size in hidden_layers:\n",
        "            layers.append(nn.Linear(current_size, layer_size))\n",
        "            layers.append(activation_fn)\n",
        "            current_size = layer_size\n",
        "        layers.append(nn.Linear(current_size, 2))  # Output layer untuk klasifikasi biner\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ],
      "metadata": {
        "id": "xtpe7oxJQeYx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagian 3: Fungsi Pelatihan dan Evaluasi\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Fungsi untuk melatih dan mengevaluasi model\n",
        "def train_and_evaluate_model(hidden_layers, activation_fn, learning_rate, batch_size, epochs):\n",
        "    # Buat DataLoader\n",
        "    train_loader = create_data_loader(X_train_tensor, y_train_tensor, batch_size)\n",
        "    test_loader = create_data_loader(X_test_tensor, y_test_tensor, batch_size)\n",
        "\n",
        "    # Inisialisasi model, loss function, dan optimizer\n",
        "    model = MLP(input_size=13, hidden_layers=hidden_layers, activation_fn=activation_fn)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Loop pelatihan\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()  # Reset gradien\n",
        "            predictions = model(X_batch)  # Forward pass\n",
        "            loss = criterion(predictions, y_batch)  # Hitung loss\n",
        "            loss.backward()  # Backpropagation\n",
        "            optimizer.step()  # Update bobot\n",
        "\n",
        "    # Evaluasi\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            predictions = model(X_batch)\n",
        "            _, predicted = torch.max(predictions, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "sStQ6Z5YQmFq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagian 4: Eksperimen Hyperparameter\n",
        "\n",
        "import itertools\n",
        "\n",
        "# Hyperparameter settings\n",
        "hidden_layer_options = [[4], [8], [16], [32], [64]]\n",
        "activation_functions = [nn.Identity(), nn.Sigmoid(), nn.ReLU(), nn.Softmax(dim=1), nn.Tanh()]\n",
        "learning_rates = [10, 1, 0.1, 0.01, 0.001]\n",
        "batch_sizes = [16, 32, 64, 128, 256]\n",
        "epoch_options = [1, 10, 25, 50, 100]\n",
        "\n",
        "# Simpan hasil\n",
        "results = []\n",
        "for hidden_layers, activation_fn, lr, batch_size, epochs in itertools.product(\n",
        "    hidden_layer_options, activation_functions, learning_rates, batch_sizes, epoch_options):\n",
        "    try:\n",
        "        accuracy = train_and_evaluate_model(\n",
        "            hidden_layers=hidden_layers,\n",
        "            activation_fn=activation_fn,\n",
        "            learning_rate=lr,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs\n",
        "        )\n",
        "        results.append({\n",
        "            'Hidden Layers': hidden_layers,\n",
        "            'Activation Function': activation_fn.__class__.__name__,\n",
        "            'Learning Rate': lr,\n",
        "            'Batch Size': batch_size,\n",
        "            'Epochs': epochs,\n",
        "            'Accuracy': accuracy\n",
        "        })\n",
        "    except Exception as e:\n",
        "        # Catat error\n",
        "        results.append({\n",
        "            'Hidden Layers': hidden_layers,\n",
        "            'Activation Function': activation_fn.__class__.__name__,\n",
        "            'Learning Rate': lr,\n",
        "            'Batch Size': batch_size,\n",
        "            'Epochs': epochs,\n",
        "            'Accuracy': None,\n",
        "            'Error': str(e)\n",
        "        })\n",
        "\n",
        "# Simpan hasil ke DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Simpan ke file CSV\n",
        "results_df.to_csv('mlp_hyperparameter_results.csv', index=False)\n",
        "print(\"Eksperimen selesai. Hasil disimpan dalam 'mlp_hyperparameter_results.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpBNR-o9Qo3s",
        "outputId": "54b3e195-833c-43c5-bd30-60cc8367313b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eksperimen selesai. Hasil disimpan dalam 'mlp_hyperparameter_results.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagian 5: Analisis Hasil\n",
        "\n",
        "# Load hasil dari CSV\n",
        "results_df = pd.read_csv('mlp_hyperparameter_results.csv')\n",
        "\n",
        "# Tampilkan 10 kombinasi terbaik berdasarkan akurasi\n",
        "top_results = results_df.sort_values(by='Accuracy', ascending=False).head(10)\n",
        "print(top_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuFxEs9LeoAB",
        "outputId": "f461422e-d73b-44b2-ef55-b4cce03dac3b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Hidden Layers Activation Function  Learning Rate  Batch Size  Epochs  \\\n",
            "2424          [32]                Tanh            1.0         256     100   \n",
            "1808          [16]                Tanh            0.1          32      50   \n",
            "1819          [16]                Tanh            0.1         128     100   \n",
            "1824          [16]                Tanh            0.1         256     100   \n",
            "1438          [16]             Sigmoid            0.1          64      50   \n",
            "1434          [16]             Sigmoid            0.1          32     100   \n",
            "1433          [16]             Sigmoid            0.1          32      50   \n",
            "1427          [16]             Sigmoid            0.1          16      25   \n",
            "2433          [32]                Tanh            0.1          32      50   \n",
            "3049          [64]                Tanh            1.0         256     100   \n",
            "\n",
            "      Accuracy  \n",
            "2424       1.0  \n",
            "1808       1.0  \n",
            "1819       1.0  \n",
            "1824       1.0  \n",
            "1438       1.0  \n",
            "1434       1.0  \n",
            "1433       1.0  \n",
            "1427       1.0  \n",
            "2433       1.0  \n",
            "3049       1.0  \n"
          ]
        }
      ]
    }
  ]
}